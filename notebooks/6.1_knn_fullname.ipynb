{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311237c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict Race/Ethnicity from Unseen Full Name Using KNN LSH MinHash Parallelized\n",
    "\n",
    "Using the Florida Voting Registration data, we build a knn classifier that predicts the ethnicity of an **unseen** name. We estimate approximate jaccard distance between names using bi-char tokens of the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b61aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import tempfile\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer                                                             \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasketch import MinHashLSHForest, MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d157d6c2-54b0-4497-a3e2-b83219c127f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Florida voter\n",
    "train_df = pd.read_csv('data/fl_2022_FullName_train.csv.gz', usecols=['full_name', 'race'])\n",
    "val_df = pd.read_csv('data/fl_2022_FullName_val.csv.gz', usecols=['full_name', 'race'])\n",
    "test_df = pd.read_csv('data/fl_2022_FullName_test.csv.gz', usecols=['full_name', 'race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51b6745-71f9-49bc-9cc5-2e61159658f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5050426, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.groupby('race', group_keys=False).apply(lambda x: x.sample(frac=.7, random_state=10))\n",
    "train_df.reset_index(inplace=True)\n",
    "train_df.drop('index', axis=1, inplace=True)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df80f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (5050426, 2)\n",
      "Validation set size: (901862, 2)\n",
      "Test set size: (901862, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Training set size: {}'.format(train_df.shape))\n",
    "print('Validation set size: {}'.format(val_df.shape))\n",
    "print('Test set size: {}'.format(test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48a8a0",
   "metadata": {},
   "source": [
    "## Find the best K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e16b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text):\n",
    "    bigrams = []\n",
    "    for i in range(len(text) - 1):\n",
    "        bigram = text[i:i + 2]\n",
    "        bigrams.append(bigram)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87b4245-9839-4ad5-ada6-82ea0f97bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minhash(row):\n",
    "    set_of_bigrams = get_bigrams(row)\n",
    "    minhash = MinHash(num_perm=num_perm)\n",
    "    for term in set_of_bigrams:\n",
    "        minhash.update(term.encode('utf-8'))\n",
    "    return minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa68350-3c34-402b-8b88-83a91ada38b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  15%|█████████████▉                                                                            | 781605/5050426 [17:59<1:47:58, 658.92row/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing rows:  74%|█████████████████████████████████████████████████████████████████▊                       | 3735104/5050426 [1:24:03<29:38, 739.38row/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing rows:  83%|██████████████████████████████████████████████████████████████████████████▏              | 4211088/5050426 [1:34:25<14:06, 991.13row/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 5050426/5050426 [1:52:28<00:00, 748.38row/s]\n"
     ]
    }
   ],
   "source": [
    "num_processes = mp.cpu_count()\n",
    "num_perm = 256  # Specify the desired number of permutations\n",
    "\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    minhashes = list(tqdm(pool.imap(create_minhash, train_df.full_name), total=train_df.shape[0], desc='Processing rows', unit='row'))\n",
    "\n",
    "# Index the forest\n",
    "forest = MinHashLSHForest(num_perm=num_perm)\n",
    "for i, minhash in enumerate(minhashes):\n",
    "    key = f\"{i}\"\n",
    "    forest.add(key, minhash)\n",
    "forest.index()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aedd8364-b4d1-4e94-a5c2-75c9cafb4680",
   "metadata": {},
   "source": [
    "# For prototyping\n",
    "# ---------------------\n",
    "# Query the forest to find the k-nearest neighbors for a given query document\n",
    "query = test_df.full_name[0]\n",
    "query_minhash = MinHash(num_perm=256)\n",
    "query_terms = get_bigrams(test_df.full_name[0])\n",
    "for j in query_terms:\n",
    "    query_minhash.update(j.encode('utf-8'))\n",
    "\n",
    "# Find the nearest neighbors using KNN search\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "result = forest.query(query_minhash, k)\n",
    "\n",
    "print(query)\n",
    "# Step 12: Print the nearest neighbors\n",
    "for key in result:\n",
    "    print(\"Nearest neighbor:\", train_df.full_name[int(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69028a9-80d7-4871-9ab1-b29e72eac5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_knn_performance(forest, test_df, k_values, batch_size=100):\n",
    "    performance = {}\n",
    "\n",
    "    for k in k_values:\n",
    "        correct_predictions = 0\n",
    "        total_examples = len(test_df)\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            num_batches = int(np.ceil(len(test_df) / batch_size))\n",
    "            batches = np.array_split(test_df, num_batches)\n",
    "            \n",
    "            for batch_idx, batch in tqdm(enumerate(batches), desc=f'Processing queries (k={k})', total=num_batches):\n",
    "                result_file = f'{temp_dir}/results_batch_{batch_idx}.pickle'\n",
    "                process_query_batch(batch, k, result_file)\n",
    "                \n",
    "                # Read batch results from disk\n",
    "                with open(result_file, 'rb') as f:\n",
    "                    batch_predicted_labels, batch_true_labels = pickle.load(f)\n",
    "                \n",
    "                predicted_labels.extend(batch_predicted_labels)\n",
    "                true_labels.extend(batch_true_labels)\n",
    "                \n",
    "                # Delete the result file after reading\n",
    "                os.remove(result_file)\n",
    "        \n",
    "        correct_predictions = sum(pred == true for pred, true in zip(predicted_labels, true_labels))\n",
    "        accuracy = correct_predictions / total_examples\n",
    "        performance[k] = accuracy\n",
    "        \n",
    "        report = classification_report(pd.Series(true_labels).replace(replacement).to_list(),\n",
    "                                           pd.Series(predicted_labels).replace(replacement).to_list(),\n",
    "                                           zero_division='warn')\n",
    "        print(f\"Classification Report (k={k}) - Batch {result_file}:\\n{report}\\n\")\n",
    "    \n",
    "\n",
    "    return performance\n",
    "\n",
    "def process_query_batch(rows, k, result_file):\n",
    "    batch_size = len(rows)\n",
    "    query_minhashes = [MinHash(num_perm=256) for _ in range(batch_size)]\n",
    "    query_terms_list = [get_bigrams(row['full_name']) for _, row in rows.iterrows()]\n",
    "    \n",
    "    for i, query_terms in enumerate(query_terms_list):\n",
    "        for term in query_terms:\n",
    "            query_minhashes[i].update(term.encode('utf-8'))\n",
    "    \n",
    "    result_batch = []\n",
    "    for query_minhash in query_minhashes:\n",
    "        result = forest.query(query_minhash, int(k))\n",
    "        result_batch.append(result)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for result, (_, row) in zip(result_batch, rows.iterrows()):\n",
    "        label_counts = {}\n",
    "\n",
    "        if result:\n",
    "            for j in result:\n",
    "                try:\n",
    "                    index = int(j)\n",
    "                    predicted_label = train_df.race[index]\n",
    "\n",
    "                    if predicted_label in label_counts:\n",
    "                        label_counts[predicted_label] += 1\n",
    "                    else:\n",
    "                        label_counts[predicted_label] = 1\n",
    "\n",
    "                except (KeyError, ValueError):\n",
    "                    continue\n",
    "\n",
    "            # Determine the predicted label based on the majority count\n",
    "            predicted_label = max(label_counts, key=label_counts.get)\n",
    "        else:\n",
    "            # Default to most frequent label from the training data if query result is empty\n",
    "            predicted_label = most_frequent_label(train_df.race)\n",
    "\n",
    "        predicted_labels.append(predicted_label)\n",
    "        true_labels.append(row['race'])\n",
    "    \n",
    "    # Write batch results to disk\n",
    "    with open(result_file, 'wb') as f:\n",
    "        pickle.dump((predicted_labels, true_labels), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99eeb584-9a1a-47ba-b04b-e26ab166d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement = {0: 'asian', 1: 'hispanic', 3: 'nh_black', 4: 'nh_white', 5: 'other'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "630eaee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries (k=10): 100%|███████████████████████████████████████████████████████████████████████████████████████| 9019/9019 [1:25:37<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (k=10) - Batch /tmp/tmpawqiqp7r/results_batch_9018.pickle:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       asian       0.64      0.18      0.28     25755\n",
      "    hispanic       0.72      0.65      0.68    163525\n",
      "    nh_black       0.58      0.31      0.40    133471\n",
      "    nh_white       0.75      0.91      0.82    552738\n",
      "       other       0.31      0.02      0.04     26373\n",
      "\n",
      "    accuracy                           0.73    901862\n",
      "   macro avg       0.60      0.41      0.44    901862\n",
      "weighted avg       0.70      0.73      0.70    901862\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries (k=25): 100%|███████████████████████████████████████████████████████████████████████████████████████| 9019/9019 [1:30:06<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (k=25) - Batch /tmp/tmp0a57yd9d/results_batch_9018.pickle:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       asian       0.70      0.13      0.22     25755\n",
      "    hispanic       0.73      0.62      0.67    163525\n",
      "    nh_black       0.63      0.24      0.35    133471\n",
      "    nh_white       0.73      0.93      0.82    552738\n",
      "       other       0.36      0.01      0.02     26373\n",
      "\n",
      "    accuracy                           0.72    901862\n",
      "   macro avg       0.63      0.39      0.42    901862\n",
      "weighted avg       0.70      0.72      0.68    901862\n",
      "\n",
      "\n",
      "{10: 0.7281368989934158, 25: 0.7246330369834852}\n",
      "CPU times: user 2h 59min 5s, sys: 0 ns, total: 2h 59min 5s\n",
      "Wall time: 2h 56min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k_values = [10, 25]\n",
    "performance = estimate_knn_performance(forest, val_df, k_values)\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a56a76ef-c3fb-4898-ac82-af6987f97254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries (k=10): 100%|███████████████████████████████████████████████████████████████████████████████████████| 9019/9019 [1:28:33<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (k=10) - Batch /tmp/tmpav_drr0h/results_batch_9018.pickle:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       asian       0.64      0.18      0.28     25756\n",
      "    hispanic       0.72      0.65      0.68    163525\n",
      "    nh_black       0.58      0.31      0.40    133471\n",
      "    nh_white       0.75      0.91      0.82    552737\n",
      "       other       0.30      0.02      0.03     26373\n",
      "\n",
      "    accuracy                           0.73    901862\n",
      "   macro avg       0.60      0.41      0.44    901862\n",
      "weighted avg       0.70      0.73      0.70    901862\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{10: 0.7275869257159078}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_knn_performance(forest, test_df, [10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d44130-c553-4e3a-bf89-da5656c3c125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
